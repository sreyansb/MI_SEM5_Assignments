{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Design of a Neural Network from scratch\n",
    "\n",
    "*************<IMP>*************\n",
    "Mention hyperparameters used and describe functionality in detail in this space\n",
    "- carries 1 mark\n",
    "'''\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "class NN: \n",
    "    parameters = list()\n",
    "    \n",
    "    def init_params(self,neuron_count_per_layer):\n",
    "        num_layers = len(neuron_count_per_layer)\n",
    "        params = [None for i in range(2*(num_layers-1))]\n",
    "        for i in range(1,num_layers):\n",
    "            params[2*i-2]=np.random.randn(neuron_count_per_layer[i], neuron_count_per_layer[i-1]) * 0.01\n",
    "            params[2*i-1]=np.zeros((neuron_count_per_layer[i], 1))\n",
    "        return params\n",
    "    \n",
    "    #Clean the data by replacing the null values with the mean of the column\n",
    "    def data_clean(self,df):\n",
    "        df.columns = df.columns.str.strip()\n",
    "        for column in df.columns:\n",
    "            if column in ['Weight','HB','BP']:\n",
    "                df[column].fillna(value=df[column].mean(), inplace=True)\n",
    "            elif column in ['Community','Delivery phase','IFA','Education']:\n",
    "                df[column].fillna(value=df[column].mode()[0], inplace=True)\n",
    "            else:\n",
    "                df[column].fillna(value=df[column].median(), inplace=True)\n",
    "        return df\n",
    "    \n",
    "    def relu(self,Z):\n",
    "        return np.maximum(0,Z),Z\n",
    "    \n",
    "    def sigmoid(self,Z):\n",
    "        return 1/(1+np.exp(-Z)),Z\n",
    "    \n",
    "    def compute_activation(self,A,weight,bias,activation):\n",
    "        Z = weight@A + bias\n",
    "        cache1 = (A,weight,bias)\n",
    "        if activation=='relu':\n",
    "            A1,cache2 = self.relu(Z)\n",
    "        else:\n",
    "            A1,cache2 = self.sigmoid(Z)\n",
    "        return A1,(cache1,cache2)\n",
    "     \n",
    "    def compute_gradients(self,dA,vals,activation):\n",
    "        cache1,cache2 = vals\n",
    "        if activation=='relu':\n",
    "            dZ = np.array(dA,copy=True)\n",
    "            #print(cache2.shape,dA.shape)\n",
    "            dZ[cache2<=0]=0\n",
    "        if activation=='sigmoid':\n",
    "            sig = 1/(1+np.exp(-cache2))\n",
    "            dZ = dA * sig * (1-sig)\n",
    "        A_prev, W, b = cache1\n",
    "        x = A_prev.shape[1]\n",
    "        dW = 1 / x * dZ @ A_prev.T\n",
    "        db = 1 / x * np.sum(dZ, axis=1, keepdims=True)\n",
    "        dA_prev = W.T @ dZ\n",
    "        return dA_prev,dW,db\n",
    "            \n",
    "    \n",
    "    def forward_propogation(self,X,parameters):\n",
    "        num_layers = len(parameters)//2\n",
    "        back_prop_values = []\n",
    "        A = X\n",
    "        \n",
    "        for i in range(1,num_layers):\n",
    "            prev_act = A\n",
    "            weight = parameters[2*i-2]\n",
    "            bias = parameters[2*i-1]\n",
    "            A,back_prop_value = self.compute_activation(prev_act,weight,bias,activation='relu')\n",
    "            back_prop_values.append(back_prop_value)\n",
    "        \n",
    "    \n",
    "        #For Last layer i.e sigmoid \n",
    "        A,back_prop_value = self.compute_activation(A,parameters[-2],parameters[-1],activation='sigmoid')\n",
    "        back_prop_values.append(back_prop_value) \n",
    "        #print(len(back_prop_values))\n",
    "        return A,back_prop_values\n",
    "    \n",
    "    def back_propogation(self,Y,activations,parameters,back_prop_values,alpha):\n",
    "        #Computing the necessary derivatives\n",
    "        gradients = {}\n",
    "        num_layers = len(parameters)//2\n",
    "        Y.reshape(activations.shape)\n",
    "        dA = - (np.divide(Y, activations) - np.divide(1 - Y, 1 - activations))\n",
    "        #print(dA.shape)\n",
    "        vals = back_prop_values[num_layers-1]\n",
    "        gradients[\"dA\" + str(num_layers-1)], gradients[\"dW\" + str(num_layers)], gradients[\"db\" + str(num_layers)] = self.compute_gradients(dA,vals,'sigmoid')\n",
    "        for layer in reversed(range(num_layers-1)):\n",
    "            vals = back_prop_values[layer]\n",
    "            gradients[\"dA\" + str(layer)], gradients[\"dW\" + str(layer + 1)], gradients[\"db\" + str(layer + 1)] = self.compute_gradients(gradients['dA'+str(layer+1)], vals, 'relu')\n",
    "        \n",
    "        #Updating the parameters\n",
    "        for i in range(1,num_layers+1):\n",
    "            parameters[2*i-2]=parameters[2*i-2] - alpha * gradients['dW'+str(i)]\n",
    "            parameters[2*i-1]=parameters[2*i-1] - alpha * gradients['db'+str(i)]\n",
    "        \n",
    "        return parameters\n",
    "        \n",
    "\n",
    "    def calc_cost(self,A,Y):\n",
    "        return np.squeeze(-1 / len(Y) * np.sum(Y * np.log(A) + (1-Y) * np.log(1-A)))\n",
    "\n",
    "    ''' X and Y are dataframes '''\n",
    "\n",
    "    def fit(self,X,Y):\n",
    "        '''\n",
    "        Function that trains the neural network by taking x_train and y_train samples as input\n",
    "        '''\n",
    "\n",
    "        #Clean the data\n",
    "        X = self.data_clean(X)\n",
    "        #Set hyperparameters\n",
    "        num_itertations = 45000\n",
    "        alpha = 0.02\n",
    "        \n",
    "    \n",
    "        #Init parameters\n",
    "        neuron_count_per_layer = [9,30,30,25,1]\n",
    "        self.parameters = self.init_params(neuron_count_per_layer)\n",
    "        \n",
    "        #Making necessary changes to dimensions\n",
    "        X = np.transpose(np.array(X))\n",
    "        Y = np.array(Y)\n",
    "        Y = np.reshape(Y,(1,Y.shape[0]))\n",
    "        \n",
    "        for i in range(1,num_itertations+1):\n",
    "            #Fp\n",
    "            activations,back_prop_values = self.forward_propogation(X,self.parameters)\n",
    "            #Bp\n",
    "            self.parameters = self.back_propogation(Y,activations,self.parameters,back_prop_values,alpha)\n",
    "            #Print Cost after every 10000 iters\n",
    "            if i%500==0:\n",
    "                print('Cost after iter '+str(i)+ ':' + str(self.calc_cost(activations,Y)/100))\n",
    " \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def predict(self,X):\n",
    "\n",
    "        \"\"\"\n",
    "        The predict function performs a simple feed forward of weights\n",
    "        and outputs yhat values \n",
    "\n",
    "        yhat is a list of the predicted value for df X\n",
    "        \"\"\"\n",
    "        yhat = []\n",
    "        X = self.data_clean(X)\n",
    "        X = np.transpose(np.array(X))\n",
    "        prob,_ = self.forward_propogation(X,self.parameters)\n",
    "        for ans in prob[0]:\n",
    "            if ans>0.5:\n",
    "                yhat.append(1)\n",
    "            else:\n",
    "                yhat.append(0)\n",
    "        return yhat\n",
    "\n",
    "    def CM(self,y_test,y_test_obs):\n",
    "        '''\n",
    "        Prints confusion matrix \n",
    "        y_test is list of y values in the test dataset\n",
    "        y_test_obs is list of y values predicted by the model\n",
    "\n",
    "        '''\n",
    "\n",
    "        for i in range(len(y_test_obs)):\n",
    "            if(y_test_obs[i]>0.6):\n",
    "                y_test_obs[i]=1\n",
    "            else:\n",
    "                y_test_obs[i]=0\n",
    "\n",
    "        cm=[[0,0],[0,0]]\n",
    "        fp=0\n",
    "        fn=0\n",
    "        tp=0\n",
    "        tn=0\n",
    "\n",
    "        for i in range(len(y_test)):\n",
    "            if(y_test[i]==1 and y_test_obs[i]==1):\n",
    "                tp=tp+1\n",
    "            if(y_test[i]==0 and y_test_obs[i]==0):\n",
    "                tn=tn+1\n",
    "            if(y_test[i]==1 and y_test_obs[i]==0):\n",
    "                fp=fp+1\n",
    "            if(y_test[i]==0 and y_test_obs[i]==1):\n",
    "                fn=fn+1\n",
    "        cm[0][0]=tn\n",
    "        cm[0][1]=fp\n",
    "        cm[1][0]=fn\n",
    "        cm[1][1]=tp\n",
    "\n",
    "        p= tp/(tp+fp)\n",
    "        r=tp/(tp+fn)\n",
    "        f1=(2*p*r)/(p+r)\n",
    "        acc = (tp+tn)/(tp+tn+fp+fn)\n",
    "        print(\"Confusion Matrix : \")\n",
    "        print(cm)\n",
    "        print(\"\\n\")\n",
    "        print(f\"Precision : {p}\")\n",
    "        print(f\"Recall : {r}\")\n",
    "        print(f\"F1 SCORE : {f1}\")\n",
    "        print(f\"Accuracy : {acc}\")\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('LBW_Dataset.csv')\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/rakshith/anaconda3/lib/python3.8/site-packages/pandas/core/generic.py:6245: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iter 500:0.4160335016745384\n",
      "Cost after iter 1000:0.41603347417266323\n",
      "Cost after iter 1500:0.4160334740459443\n",
      "Cost after iter 2000:0.416033473934557\n",
      "Cost after iter 2500:0.41603347382463307\n",
      "Cost after iter 3000:0.41603347369585053\n",
      "Cost after iter 3500:0.4160334735844113\n",
      "Cost after iter 4000:0.4160334734794684\n",
      "Cost after iter 4500:0.4160334733707217\n",
      "Cost after iter 5000:0.41603347326083956\n",
      "Cost after iter 5500:0.4160334731493321\n",
      "Cost after iter 6000:0.41603347303711985\n",
      "Cost after iter 6500:0.4160334729282978\n",
      "Cost after iter 7000:0.41603347281903214\n",
      "Cost after iter 7500:0.41603347270800517\n",
      "Cost after iter 8000:0.41603347259548085\n",
      "Cost after iter 8500:0.41603347248182354\n",
      "Cost after iter 9000:0.4160334723669667\n",
      "Cost after iter 9500:0.4160334722495346\n",
      "Cost after iter 10000:0.4160334721318543\n",
      "Cost after iter 10500:0.4160334720149107\n",
      "Cost after iter 11000:0.4160334719003345\n",
      "Cost after iter 11500:0.41603347178474737\n",
      "Cost after iter 12000:0.4160334716697549\n",
      "Cost after iter 12500:0.4160334715526529\n",
      "Cost after iter 13000:0.41603347143602976\n",
      "Cost after iter 13500:0.4160334713176204\n",
      "Cost after iter 14000:0.4160334711970015\n",
      "Cost after iter 14500:0.4160334710767004\n",
      "Cost after iter 15000:0.41603347095612214\n",
      "Cost after iter 15500:0.41603347083210424\n",
      "Cost after iter 16000:0.41603347070604035\n",
      "Cost after iter 16500:0.41603347057858864\n",
      "Cost after iter 17000:0.41603347044926603\n",
      "Cost after iter 17500:0.41603347031985416\n",
      "Cost after iter 18000:0.41603347019526665\n",
      "Cost after iter 18500:0.41603347007012864\n",
      "Cost after iter 19000:0.4160334699452083\n",
      "Cost after iter 19500:0.4160334698183192\n",
      "Cost after iter 20000:0.41603346969131083\n",
      "Cost after iter 20500:0.41603346956229253\n",
      "Cost after iter 21000:0.4160334694334134\n",
      "Cost after iter 21500:0.41603346930299456\n",
      "Cost after iter 22000:0.4160334691687884\n",
      "Cost after iter 22500:0.41603346903732713\n",
      "Cost after iter 23000:0.41603346890490867\n",
      "Cost after iter 23500:0.41603346877093017\n",
      "Cost after iter 24000:0.4160334686379595\n",
      "Cost after iter 24500:0.4160334685042302\n",
      "Cost after iter 25000:0.41603346837015714\n",
      "Cost after iter 25500:0.4160334682355632\n",
      "Cost after iter 26000:0.4160334680994979\n",
      "Cost after iter 26500:0.41603346796110136\n",
      "Cost after iter 27000:0.41603346782166395\n",
      "Cost after iter 27500:0.4160334676812571\n",
      "Cost after iter 28000:0.4160334675376789\n",
      "Cost after iter 28500:0.41603346738758307\n",
      "Cost after iter 29000:0.41603346723676277\n",
      "Cost after iter 29500:0.41603346708251576\n",
      "Cost after iter 30000:0.41603346692579224\n",
      "Cost after iter 30500:0.4160334667697642\n",
      "Cost after iter 31000:0.4160334666119189\n",
      "Cost after iter 31500:0.4160334664527582\n",
      "Cost after iter 32000:0.4160334662930499\n",
      "Cost after iter 32500:0.4160334661315133\n",
      "Cost after iter 33000:0.41603346596868157\n",
      "Cost after iter 33500:0.41603346580474415\n",
      "Cost after iter 34000:0.41603346563964566\n",
      "Cost after iter 34500:0.4160334654734412\n",
      "Cost after iter 35000:0.4160334653043293\n",
      "Cost after iter 35500:0.4160334651316846\n",
      "Cost after iter 36000:0.4160334649516113\n",
      "Cost after iter 36500:0.41603346476515596\n",
      "Cost after iter 37000:0.4160334645755097\n",
      "Cost after iter 37500:0.4160334643645889\n",
      "Cost after iter 38000:0.41603346416350695\n",
      "Cost after iter 38500:0.4160334639763288\n",
      "Cost after iter 39000:0.41603346378852235\n",
      "Cost after iter 39500:0.41603346359918597\n",
      "Cost after iter 40000:0.41603346340443986\n",
      "Cost after iter 40500:0.4160334632090107\n",
      "Cost after iter 41000:0.4160334630111076\n",
      "Cost after iter 41500:0.41603346281068804\n",
      "Cost after iter 42000:0.41603346260818014\n",
      "Cost after iter 42500:0.41603346240142336\n",
      "Cost after iter 43000:0.4160334621904394\n",
      "Cost after iter 43500:0.41603346197934\n",
      "Cost after iter 44000:0.41603346176806333\n",
      "Cost after iter 44500:0.41603346155520815\n",
      "Cost after iter 45000:0.4160334613401748\n",
      "Cost after iter 45500:0.41603346112296763\n",
      "Cost after iter 46000:0.4160334609029017\n",
      "Cost after iter 46500:0.41603346068122027\n",
      "Cost after iter 47000:0.4160334604573492\n",
      "Cost after iter 47500:0.4160334602319493\n",
      "Cost after iter 48000:0.41603345999980623\n",
      "Cost after iter 48500:0.41603345976401807\n",
      "Cost after iter 49000:0.4160334595109426\n",
      "Cost after iter 49500:0.41603345926872637\n",
      "Cost after iter 50000:0.41603345902447053\n",
      "Cost after iter 50500:0.4160334587731308\n",
      "Cost after iter 51000:0.4160334585166754\n",
      "Cost after iter 51500:0.41603345825948024\n",
      "Cost after iter 52000:0.41603345800125247\n",
      "Cost after iter 52500:0.41603345774127776\n",
      "Cost after iter 53000:0.41603345747751336\n",
      "Cost after iter 53500:0.4160334572110078\n",
      "Cost after iter 54000:0.41603345694250854\n",
      "Cost after iter 54500:0.4160334566701149\n",
      "Cost after iter 55000:0.4160334563938068\n",
      "Cost after iter 55500:0.4160334561146833\n",
      "Cost after iter 56000:0.4160334558313385\n",
      "Cost after iter 56500:0.4160334555436943\n",
      "Cost after iter 57000:0.41603345525166274\n",
      "Cost after iter 57500:0.4160334549551478\n",
      "Cost after iter 58000:0.4160334546540501\n",
      "Cost after iter 58500:0.4160334543482713\n",
      "Cost after iter 59000:0.4160334540377175\n",
      "Cost after iter 59500:0.41603345372230366\n",
      "Cost after iter 60000:0.41603345340189163\n",
      "Cost after iter 60500:0.416033453076368\n",
      "Cost after iter 61000:0.4160334527458829\n",
      "Cost after iter 61500:0.41603345241136014\n",
      "Cost after iter 62000:0.4160334520708504\n",
      "Cost after iter 62500:0.41603345172469164\n",
      "Cost after iter 63000:0.4160334513738374\n",
      "Cost after iter 63500:0.4160334510178615\n",
      "Cost after iter 64000:0.41603345065602476\n",
      "Cost after iter 64500:0.4160334502882097\n",
      "Cost after iter 65000:0.41603344991424424\n",
      "Cost after iter 65500:0.4160334495340041\n",
      "Cost after iter 66000:0.4160334491473452\n",
      "Cost after iter 66500:0.4160334487541093\n",
      "Cost after iter 67000:0.4160334483541568\n",
      "Cost after iter 67500:0.4160334479479823\n",
      "Cost after iter 68000:0.4160334475355547\n",
      "Cost after iter 68500:0.416033447115928\n",
      "Cost after iter 69000:0.41603344668598374\n",
      "Cost after iter 69500:0.4160334462479749\n",
      "Cost after iter 70000:0.4160334458021145\n",
      "Cost after iter 70500:0.4160334453482045\n",
      "Cost after iter 71000:0.4160334448860773\n",
      "Cost after iter 71500:0.4160334444155038\n",
      "Cost after iter 72000:0.41603344393259895\n",
      "Cost after iter 72500:0.4160334434377863\n",
      "Cost after iter 73000:0.4160334429346566\n",
      "Cost after iter 73500:0.4160334424219665\n",
      "Cost after iter 74000:0.41603344189947444\n",
      "Cost after iter 74500:0.41603344135938713\n",
      "Cost after iter 75000:0.4160334408099206\n",
      "Cost after iter 75500:0.41603344025025585\n",
      "Cost after iter 76000:0.41603343968001966\n",
      "Cost after iter 76500:0.41603343909841634\n",
      "Cost after iter 77000:0.41603343850511526\n",
      "Cost after iter 77500:0.4160334378998652\n",
      "Cost after iter 78000:0.4160334372824142\n",
      "Cost after iter 78500:0.4160334366521872\n",
      "Cost after iter 79000:0.41603343601077225\n",
      "Cost after iter 79500:0.41603343535966736\n",
      "Cost after iter 80000:0.4160334346951228\n",
      "Cost after iter 80500:0.4160334340166555\n",
      "Cost after iter 81000:0.416033433323919\n",
      "Cost after iter 81500:0.41603343261649506\n",
      "Cost after iter 82000:0.41603343189665337\n",
      "Cost after iter 82500:0.41603343116821384\n",
      "Cost after iter 83000:0.4160334304165387\n",
      "Cost after iter 83500:0.41603342964834966\n",
      "Cost after iter 84000:0.41603342886292694\n",
      "Cost after iter 84500:0.4160334280457702\n",
      "Cost after iter 85000:0.416033427213558\n",
      "Cost after iter 85500:0.4160334263640854\n",
      "Cost after iter 86000:0.41603342549560424\n",
      "Cost after iter 86500:0.4160334246074192\n",
      "Cost after iter 87000:0.4160334237034084\n",
      "Cost after iter 87500:0.41603342277936134\n",
      "Cost after iter 88000:0.4160334218168081\n",
      "Cost after iter 88500:0.41603342080948613\n",
      "Cost after iter 89000:0.41603341979132114\n",
      "Cost after iter 89500:0.4160334187606687\n",
      "Cost after iter 90000:0.41603341771420643\n",
      "Cost after iter 90500:0.4160334166486211\n",
      "Cost after iter 91000:0.4160334155570833\n",
      "Cost after iter 91500:0.4160334144410853\n",
      "Cost after iter 92000:0.41603341330268984\n",
      "Cost after iter 92500:0.41603341205942146\n",
      "Cost after iter 93000:0.4160334108556937\n",
      "Cost after iter 93500:0.41603340962080165\n",
      "Cost after iter 94000:0.41603340836132424\n",
      "Cost after iter 94500:0.4160334070704461\n",
      "Cost after iter 95000:0.4160334057303388\n",
      "Cost after iter 95500:0.41603340437060154\n",
      "Cost after iter 96000:0.4160334029755253\n",
      "Cost after iter 96500:0.4160334015422001\n",
      "Cost after iter 97000:0.4160334000718235\n",
      "Cost after iter 97500:0.4160333985632842\n",
      "Cost after iter 98000:0.416033397011009\n",
      "Cost after iter 98500:0.41603339530610783\n",
      "Cost after iter 99000:0.41603339365050246\n",
      "Cost after iter 99500:0.4160333919343601\n",
      "Cost after iter 100000:0.4160333901537374\n",
      "Cost after iter 100500:0.416033388302516\n",
      "Cost after iter 101000:0.416033386204048\n",
      "Cost after iter 101500:0.4160333841855495\n",
      "Cost after iter 102000:0.4160333821821751\n",
      "Cost after iter 102500:0.41603338013570673\n",
      "Cost after iter 103000:0.41603337803445994\n",
      "Cost after iter 103500:0.41603337586816347\n",
      "Cost after iter 104000:0.4160333736454613\n",
      "Cost after iter 104500:0.41603337050593003\n",
      "Cost after iter 105000:0.41603336791590273\n",
      "Cost after iter 105500:0.41603336520079864\n",
      "Cost after iter 106000:0.41603336243368766\n",
      "Cost after iter 106500:0.4160333596962476\n",
      "Cost after iter 107000:0.41603335677451275\n",
      "Cost after iter 107500:0.4160333536985448\n",
      "Cost after iter 108000:0.4160333504706815\n",
      "Cost after iter 108500:0.4160333473013138\n",
      "Cost after iter 109000:0.41603334402505127\n",
      "Cost after iter 109500:0.41603334049293855\n",
      "Cost after iter 110000:0.41603333673733794\n",
      "Cost after iter 110500:0.416033333013364\n",
      "Cost after iter 111000:0.4160333292087979\n",
      "Cost after iter 111500:0.41603332534416476\n",
      "Cost after iter 112000:0.4160333213916852\n",
      "Cost after iter 112500:0.4160333172891134\n",
      "Cost after iter 113000:0.41603331302847957\n",
      "Cost after iter 113500:0.41603330860115884\n",
      "Cost after iter 114000:0.41603330399909133\n",
      "Cost after iter 114500:0.4160332992119797\n",
      "Cost after iter 115000:0.41603329423098023\n",
      "Cost after iter 115500:0.41603328904384496\n",
      "Cost after iter 116000:0.4160332836397808\n",
      "Cost after iter 116500:0.41603327800599843\n",
      "Cost after iter 117000:0.4160332721293954\n",
      "Cost after iter 117500:0.41603326599476864\n",
      "Cost after iter 118000:0.4160332595862907\n",
      "Cost after iter 118500:0.4160332528927985\n",
      "Cost after iter 119000:0.4160332458915533\n",
      "Cost after iter 119500:0.4160332385623442\n",
      "Cost after iter 120000:0.4160332308842127\n",
      "Cost after iter 120500:0.416033222834028\n",
      "Cost after iter 121000:0.416033214387685\n",
      "Cost after iter 121500:0.416033205517509\n",
      "Cost after iter 122000:0.4160331961938769\n",
      "Cost after iter 122500:0.4160331863856237\n",
      "Cost after iter 123000:0.4160331760576594\n",
      "Cost after iter 123500:0.4160331651712584\n",
      "Cost after iter 124000:0.4160331536860483\n",
      "Cost after iter 124500:0.41603314155664817\n",
      "Cost after iter 125000:0.41603312873149184\n",
      "Cost after iter 125500:0.4160331151550238\n",
      "Cost after iter 126000:0.4160331007676142\n",
      "Cost after iter 126500:0.41603308550030454\n",
      "Cost after iter 127000:0.416033069278844\n",
      "Cost after iter 127500:0.41603305200434904\n",
      "Cost after iter 128000:0.4160330335932041\n",
      "Cost after iter 128500:0.41603301393333303\n",
      "Cost after iter 129000:0.4160329929269798\n",
      "Cost after iter 129500:0.4160329704404765\n",
      "Cost after iter 130000:0.4160329463292566\n",
      "Cost after iter 130500:0.41603292042157536\n",
      "Cost after iter 131000:0.4160328925527892\n",
      "Cost after iter 131500:0.4160328625198057\n",
      "Cost after iter 132000:0.4160328300656824\n",
      "Cost after iter 132500:0.416032794915468\n",
      "Cost after iter 133000:0.41603275675530654\n",
      "Cost after iter 133500:0.4160327152257129\n",
      "Cost after iter 134000:0.4160326699094358\n",
      "Cost after iter 134500:0.41603262031434596\n",
      "Cost after iter 135000:0.4160325651460678\n",
      "Cost after iter 135500:0.41603250486624416\n",
      "Cost after iter 136000:0.4160324384051918\n",
      "Cost after iter 136500:0.4160323642598675\n",
      "Cost after iter 137000:0.4160322813914748\n",
      "Cost after iter 137500:0.4160321888763906\n",
      "Cost after iter 138000:0.4160320853630514\n",
      "Cost after iter 138500:0.4160319676996548\n",
      "Cost after iter 139000:0.41603183329909316\n",
      "Cost after iter 139500:0.41603167749908065\n",
      "Cost after iter 140000:0.4160314994815292\n",
      "Cost after iter 140500:0.4160312943337386\n",
      "Cost after iter 141000:0.4160310545015867\n",
      "Cost after iter 141500:0.4160307710467918\n",
      "Cost after iter 142000:0.41603043226303016\n",
      "Cost after iter 142500:0.4160300221677949\n",
      "Cost after iter 143000:0.41602951851189834\n",
      "Cost after iter 143500:0.41602888928473986\n",
      "Cost after iter 144000:0.416028087528787\n",
      "Cost after iter 144500:0.4160270311300656\n",
      "Cost after iter 145000:0.4160256118757\n",
      "Cost after iter 145500:0.41602363069654347\n",
      "Cost after iter 146000:0.4160207298486575\n",
      "Cost after iter 146500:0.41601620940378675\n",
      "Cost after iter 147000:0.4160085065015068\n",
      "Cost after iter 147500:0.41599347734045566\n",
      "Cost after iter 148000:0.41595646837164596\n",
      "Cost after iter 148500:0.4158058908496236\n",
      "Cost after iter 149000:0.4126957266559863\n",
      "Cost after iter 149500:0.3788761966326177\n",
      "Cost after iter 150000:0.368479698280451\n",
      "Cost after iter 150500:0.3693611974191018\n",
      "Cost after iter 151000:0.3625949649028761\n",
      "Cost after iter 151500:0.3547059823941608\n",
      "Cost after iter 152000:0.355302529739849\n",
      "Cost after iter 152500:0.35327578728435927\n",
      "Cost after iter 153000:0.35224223459298676\n",
      "Cost after iter 153500:0.3457409185604368\n",
      "Cost after iter 154000:0.3461240984466915\n",
      "Cost after iter 154500:0.34300285536913655\n",
      "Cost after iter 155000:0.3432092386572328\n",
      "Cost after iter 155500:0.34273291002433603\n",
      "Cost after iter 156000:0.33516819697492023\n",
      "Cost after iter 156500:0.33658510968975963\n",
      "Cost after iter 157000:0.34327983037220056\n",
      "Cost after iter 157500:0.33849638587184955\n",
      "Cost after iter 158000:0.3381387271950864\n",
      "Cost after iter 158500:0.33670311920604284\n",
      "Cost after iter 159000:0.3371373341907755\n",
      "Cost after iter 159500:0.3368182336690422\n",
      "Cost after iter 160000:0.3364546642141615\n",
      "Cost after iter 160500:0.33577102036984174\n",
      "Cost after iter 161000:0.33558648276037706\n",
      "Cost after iter 161500:0.3351248168164346\n",
      "Cost after iter 162000:0.3346796909266351\n",
      "Cost after iter 162500:0.3341431819632452\n",
      "Cost after iter 163000:0.3337002627685115\n",
      "Cost after iter 163500:0.33315662251063827\n",
      "Cost after iter 164000:0.33272226740762145\n",
      "Cost after iter 164500:0.3322132642808599\n",
      "Cost after iter 165000:0.3317737850840462\n",
      "Cost after iter 165500:0.3313539872670107\n",
      "Cost after iter 166000:0.3308498342319688\n",
      "Cost after iter 166500:0.33032188703310567\n",
      "Cost after iter 167000:0.329776471399996\n",
      "Cost after iter 167500:0.3280094106012414\n",
      "Cost after iter 168000:0.3232705400611625\n",
      "Cost after iter 168500:0.334971577475784\n",
      "Cost after iter 169000:0.3195684528120972\n",
      "Cost after iter 169500:0.32668600414562504\n",
      "Cost after iter 170000:0.31885127769551175\n",
      "Cost after iter 170500:0.3246097542048038\n",
      "Cost after iter 171000:0.33338584549112016\n",
      "Cost after iter 171500:0.3272578408994461\n",
      "Cost after iter 172000:0.3256587562525267\n",
      "Cost after iter 172500:0.32225785931868794\n",
      "Cost after iter 173000:0.32394122670821923\n",
      "Cost after iter 173500:0.32560053813605166\n",
      "Cost after iter 174000:0.32440768491408156\n",
      "Cost after iter 174500:0.32235909994264206\n",
      "Cost after iter 175000:0.31737734719365196\n",
      "Cost after iter 175500:0.33427154274441434\n",
      "Cost after iter 176000:0.3240454491003632\n",
      "Cost after iter 176500:0.3072202412932698\n",
      "Cost after iter 177000:0.307991406005478\n",
      "Cost after iter 177500:0.307587067703451\n",
      "Cost after iter 178000:0.344103213967917\n",
      "Cost after iter 178500:0.3746172707383703\n",
      "Cost after iter 179000:0.3463986292413201\n",
      "Cost after iter 179500:0.4335674869488449\n",
      "Cost after iter 180000:0.4160334821046684\n",
      "Cost after iter 180500:0.4160334760793816\n",
      "Cost after iter 181000:0.4160334760793788\n",
      "Cost after iter 181500:0.4160334760793788\n",
      "Cost after iter 182000:0.41603347607937885\n",
      "Cost after iter 182500:0.41603347607937885\n",
      "Cost after iter 183000:0.41603347607937885\n",
      "Cost after iter 183500:0.41603347607937885\n",
      "Cost after iter 184000:0.41603347607937885\n",
      "Cost after iter 184500:0.41603347607937885\n",
      "Cost after iter 185000:0.41603347607937885\n"
     ]
    }
   ],
   "source": [
    "nn.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_hat = nn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix : \n",
      "[[0, 0], [6, 14]]\n",
      "\n",
      "\n",
      "Precision : 1.0\n",
      "Recall : 0.7\n",
      "F1 SCORE : 0.8235294117647058\n",
      "Accuracy : 0.7\n"
     ]
    }
   ],
   "source": [
    "nn.CM(y_test.tolist(),y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Community</th>\n",
       "      <th>Age</th>\n",
       "      <th>Weight</th>\n",
       "      <th>Delivery phase</th>\n",
       "      <th>HB</th>\n",
       "      <th>IFA</th>\n",
       "      <th>BP</th>\n",
       "      <th>Education</th>\n",
       "      <th>Residence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>1</td>\n",
       "      <td>24.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.900000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.666667</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>3</td>\n",
       "      <td>24.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.714286</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>1</td>\n",
       "      <td>25.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.139286</td>\n",
       "      <td>0</td>\n",
       "      <td>1.804353</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>3</td>\n",
       "      <td>21.0</td>\n",
       "      <td>55.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.375000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>3</td>\n",
       "      <td>24.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8.400000</td>\n",
       "      <td>0</td>\n",
       "      <td>1.500000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>43.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.571429</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>52.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.800000</td>\n",
       "      <td>1</td>\n",
       "      <td>13.875000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1</td>\n",
       "      <td>26.0</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.571000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>21.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.200000</td>\n",
       "      <td>1</td>\n",
       "      <td>1.571000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>67 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Community   Age  Weight  Delivery phase         HB  IFA         BP  \\\n",
       "75          1  24.0    43.0             1.0   9.000000    1   1.666667   \n",
       "60          1  21.0    40.0             1.0   7.900000    0   1.666667   \n",
       "79          3  24.0    60.0             1.0   9.200000    1   1.714286   \n",
       "85          1  25.0    52.0             1.0   9.139286    0   1.804353   \n",
       "91          3  21.0    55.0             1.0   9.000000    0   1.375000   \n",
       "..        ...   ...     ...             ...        ...  ...        ...   \n",
       "92          3  24.0    39.0             2.0   8.400000    0   1.500000   \n",
       "67          1  21.0    43.0             1.0  10.200000    1   1.571429   \n",
       "64          1  21.0    52.0             1.0   8.800000    1  13.875000   \n",
       "47          1  26.0    35.0             1.0   9.200000    1   1.571000   \n",
       "44          1  21.0    40.0             1.0   9.200000    1   1.571000   \n",
       "\n",
       "    Education  Residence  \n",
       "75        5.0        2.0  \n",
       "60        5.0        1.0  \n",
       "79        5.0        1.0  \n",
       "85        5.0        1.0  \n",
       "91        5.0        1.0  \n",
       "..        ...        ...  \n",
       "92        5.0        1.0  \n",
       "67        5.0        1.0  \n",
       "64        5.0        2.0  \n",
       "47        5.0        1.0  \n",
       "44        5.0        1.0  \n",
       "\n",
       "[67 rows x 9 columns]"
      ]
     },
     "execution_count": 279,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iter 500:0.3697503329495725\n",
      "Cost after iter 1000:0.36833216036809424\n",
      "Cost after iter 1500:0.3682983110880379\n",
      "Cost after iter 2000:0.36829728810047707\n",
      "Cost after iter 2500:0.368297091951979\n",
      "Cost after iter 3000:0.36829691122891717\n",
      "Cost after iter 3500:0.36829671007761255\n",
      "Cost after iter 4000:0.3682964837717205\n",
      "Cost after iter 4500:0.368296227763276\n",
      "Cost after iter 5000:0.36829593635238916\n",
      "Cost after iter 5500:0.3682956033342288\n",
      "Cost after iter 6000:0.3682952203526847\n",
      "Cost after iter 6500:0.36829477614278167\n",
      "Cost after iter 7000:0.36829425413251904\n",
      "Cost after iter 7500:0.3682936203894517\n",
      "Cost after iter 8000:0.36829284514929844\n",
      "Cost after iter 8500:0.3682919209413844\n",
      "Cost after iter 9000:0.3682907793019294\n",
      "Cost after iter 9500:0.368289381521606\n",
      "Cost after iter 10000:0.3682876423572414\n",
      "Cost after iter 10500:0.3682854205741322\n",
      "Cost after iter 11000:0.36828251493211694\n",
      "Cost after iter 11500:0.3682786322885097\n",
      "Cost after iter 12000:0.36827326512423025\n",
      "Cost after iter 12500:0.36826555869583105\n",
      "Cost after iter 13000:0.3682539437809523\n",
      "Cost after iter 13500:0.3682352871257342\n",
      "Cost after iter 14000:0.3682024050086485\n",
      "Cost after iter 14500:0.36813753377383696\n",
      "Cost after iter 15000:0.3679835759287113\n",
      "Cost after iter 15500:0.36751330654075126\n",
      "Cost after iter 16000:0.36597507839080706\n",
      "Cost after iter 16500:0.3622946923675587\n",
      "Cost after iter 17000:0.34655176538806637\n",
      "Cost after iter 17500:0.33619158473709787\n",
      "Cost after iter 18000:0.3284451522012112\n",
      "Cost after iter 18500:0.3249147075290736\n",
      "Cost after iter 19000:0.3245499204603031\n",
      "Cost after iter 19500:0.3215153273948666\n",
      "Cost after iter 20000:0.3208989721820508\n",
      "Cost after iter 20500:0.31999363534810477\n",
      "Cost after iter 21000:0.3200190693502728\n",
      "Cost after iter 21500:0.3162947444144206\n",
      "Cost after iter 22000:0.31533697887310863\n",
      "Cost after iter 22500:0.31504419820468277\n",
      "Cost after iter 23000:0.3146469722568587\n",
      "Cost after iter 23500:0.3139867826701281\n",
      "Cost after iter 24000:0.3180898357914939\n",
      "Cost after iter 24500:0.313275339576861\n",
      "Cost after iter 25000:0.3129143034451197\n",
      "Cost after iter 25500:0.3131602135203513\n",
      "Cost after iter 26000:0.31462094013546926\n",
      "Cost after iter 26500:0.3120999745176971\n",
      "Cost after iter 27000:0.3120011139346543\n",
      "Cost after iter 27500:0.3117287189439067\n",
      "Cost after iter 28000:0.3233088512415545\n",
      "Cost after iter 28500:0.31318331761565793\n",
      "Cost after iter 29000:0.31484269872941545\n",
      "Cost after iter 29500:0.31183600814952733\n",
      "Cost after iter 30000:0.3112717951390113\n",
      "Cost after iter 30500:0.3130937949199765\n",
      "Cost after iter 31000:0.3128036660339987\n",
      "Cost after iter 31500:0.31285660561316236\n",
      "Cost after iter 32000:0.31255083229727626\n",
      "Cost after iter 32500:0.3121375342668531\n",
      "Cost after iter 33000:0.31093493568087005\n",
      "Cost after iter 33500:0.3103474334650432\n",
      "Cost after iter 34000:0.31093954313931765\n",
      "Cost after iter 34500:0.30904568198988197\n",
      "Cost after iter 35000:0.30828185271292846\n",
      "Cost after iter 35500:0.30089350002502957\n",
      "Cost after iter 36000:0.2960113397208789\n",
      "Cost after iter 36500:0.2889093847825939\n",
      "Cost after iter 37000:0.27761723431974644\n",
      "Cost after iter 37500:0.2999825763371792\n",
      "Cost after iter 38000:0.2786881714093481\n",
      "Cost after iter 38500:0.2651126432281281\n",
      "Cost after iter 39000:0.2764893207110654\n",
      "Cost after iter 39500:0.266041063393339\n",
      "Cost after iter 40000:0.2774996237708388\n",
      "Cost after iter 40500:0.2507614289807339\n",
      "Cost after iter 41000:0.2555040131047219\n",
      "Cost after iter 41500:0.25287851576621134\n",
      "Cost after iter 42000:0.26152187669869703\n",
      "Cost after iter 42500:0.2552614213627134\n",
      "Cost after iter 43000:0.23174236496729045\n",
      "Cost after iter 43500:0.2342800140379543\n",
      "Cost after iter 44000:0.25686806566656456\n",
      "Cost after iter 44500:0.22692454568549802\n",
      "Cost after iter 45000:0.2631301594065836\n"
     ]
    }
   ],
   "source": [
    "nn1 = NN()\n",
    "nn1.fit(X_train,y_train)\n",
    "y_hat1 = nn1.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix : \n",
      "[[4, 0], [4, 21]]\n",
      "\n",
      "\n",
      "Precision : 1.0\n",
      "Recall : 0.84\n",
      "F1 SCORE : 0.9130434782608696\n",
      "Accuracy : 0.8620689655172413\n"
     ]
    }
   ],
   "source": [
    "nn1.CM(y_test.tolist(),y_hat1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
