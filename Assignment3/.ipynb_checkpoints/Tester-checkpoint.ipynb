{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Design of a Neural Network from scratch\n",
    "\n",
    "*************<IMP>*************\n",
    "Mention hyperparameters used and describe functionality in detail in this space\n",
    "- carries 1 mark\n",
    "'''\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class NN: \n",
    "    parameters = list()\n",
    "    \n",
    "    def init_params(self,neuron_count_per_layer):\n",
    "        num_layers = len(neuron_count_per_layer)\n",
    "        params = [None for i in range(2*(num_layers-1))]\n",
    "        for i in range(1,num_layers):\n",
    "            params[2*i-2]=np.random.randn(neuron_count_per_layer[i], neuron_count_per_layer[i-1]) * 0.15\n",
    "            params[2*i-1]=np.zeros((neuron_count_per_layer[i], 1))\n",
    "        #print(params)\n",
    "        return params\n",
    "    \n",
    "    #Clean the data by replacing the null values with the mean/median/mode of the column\n",
    "    def data_clean(self,df):\n",
    "        df.columns = df.columns.str.strip()\n",
    "        for column in df.columns:\n",
    "            if column in ['Weight','HB','BP']:\n",
    "                df[column].fillna(value=df[column].mean(), inplace=True)\n",
    "            elif column in ['Community','Delivery phase','IFA','Education']:\n",
    "                df[column].fillna(value=df[column].mode()[0], inplace=True)\n",
    "            else:\n",
    "                df[column].fillna(value=df[column].median(), inplace=True)\n",
    "        return df\n",
    "    \n",
    "    def relu(self,Z):\n",
    "        return np.maximum(0,Z),Z\n",
    "    \n",
    "    def sigmoid(self,Z):\n",
    "        return 1/(1+np.exp(-Z)),Z\n",
    "    \n",
    "    def compute_activation(self,A,weight,bias,activation):\n",
    "        Z = weight@A + bias\n",
    "        cache1 = (A,weight,bias)\n",
    "        if activation=='relu':\n",
    "            A1,cache2 = self.relu(Z)\n",
    "        else:\n",
    "            A1,cache2 = self.sigmoid(Z)\n",
    "        return A1,(cache1,cache2)\n",
    "     \n",
    "    def compute_gradients(self,dA,vals,activation):\n",
    "        cache1,cache2 = vals\n",
    "        if activation=='relu':\n",
    "            dZ = np.array(dA,copy=True)\n",
    "            #print(cache2.shape,dA.shape)\n",
    "            dZ[cache2<=0]=0\n",
    "        if activation=='sigmoid':\n",
    "            sig = 1/(1+np.exp(-cache2))\n",
    "            dZ = dA * sig * (1-sig)\n",
    "        A_prev, W, b = cache1\n",
    "        x = A_prev.shape[1]\n",
    "        dW = 1 / x * dZ @ A_prev.T\n",
    "        db = 1 / x * np.sum(dZ, axis=1, keepdims=True)\n",
    "        dA_prev = W.T @ dZ\n",
    "        return dA_prev,dW,db\n",
    "            \n",
    "    \n",
    "    def forward_propogation(self,X,parameters):\n",
    "        num_layers = len(parameters)//2\n",
    "        back_prop_values = []\n",
    "        A = X\n",
    "        \n",
    "        for i in range(1,num_layers):\n",
    "            prev_act = A\n",
    "            weight = parameters[2*i-2]\n",
    "            bias = parameters[2*i-1]\n",
    "            A,back_prop_value = self.compute_activation(prev_act,weight,bias,activation='relu')\n",
    "            back_prop_values.append(back_prop_value)\n",
    "        \n",
    "    \n",
    "        #For Last layer i.e sigmoid \n",
    "        A,back_prop_value = self.compute_activation(A,parameters[-2],parameters[-1],activation='sigmoid')\n",
    "        back_prop_values.append(back_prop_value) \n",
    "        #print(len(back_prop_values))\n",
    "        return A,back_prop_values\n",
    "    \n",
    "    def back_propogation(self,Y,activations,parameters,back_prop_values,alpha):\n",
    "        #Computing the necessary derivatives\n",
    "        gradients = {}\n",
    "        num_layers = len(parameters)//2\n",
    "        Y.reshape(activations.shape)\n",
    "        dA = - (np.divide(Y, activations) - np.divide(1 - Y, 1 - activations))\n",
    "        #print(dA.shape)\n",
    "        vals = back_prop_values[num_layers-1]\n",
    "        gradients[\"dA\" + str(num_layers-1)], gradients[\"dW\" + str(num_layers)], gradients[\"db\" + str(num_layers)] = self.compute_gradients(dA,vals,'sigmoid')\n",
    "        for layer in reversed(range(num_layers-1)):\n",
    "            vals = back_prop_values[layer]\n",
    "            gradients[\"dA\" + str(layer)], gradients[\"dW\" + str(layer + 1)], gradients[\"db\" + str(layer + 1)] = self.compute_gradients(gradients['dA'+str(layer+1)], vals, 'relu')\n",
    "        \n",
    "        #Updating the parameters\n",
    "        for i in range(1,num_layers+1):\n",
    "            \n",
    "            #Compute v\n",
    "            \n",
    "            parameters[2*i-2]=parameters[2*i-2] - alpha * gradients['dW'+str(i)]\n",
    "            parameters[2*i-1]=parameters[2*i-1] - alpha * gradients['db'+str(i)]  \n",
    "        return parameters\n",
    "        \n",
    "\n",
    "    def calc_cost(self,A,Y):\n",
    "        return np.squeeze(-1 / len(Y) * np.sum(Y * np.log(A) + (1-Y) * np.log(1-A)))\n",
    "\n",
    "    ''' X and Y are dataframes '''\n",
    "\n",
    "    def fit(self,X,Y):\n",
    "        '''\n",
    "        Function that trains the neural network by taking x_train and y_train samples as input\n",
    "        '''\n",
    "        np.random.seed(10)\n",
    "        #Clean the data\n",
    "        X = self.data_clean(X)\n",
    "        #Set hyperparameters\n",
    "        num_itertations = 10000\n",
    "        alpha = 0.01\n",
    "        neuron_count_per_layer = [9,30,30,25,1]\n",
    "    \n",
    "        #Init parameters\n",
    "        self.parameters = self.init_params(neuron_count_per_layer)\n",
    "        \n",
    "        #Making necessary changes to dimensions\n",
    "        X = np.transpose(np.array(X))\n",
    "        Y = np.array(Y)\n",
    "        Y = np.reshape(Y,(1,Y.shape[0]))\n",
    "        \n",
    "        for i in range(1,num_itertations+1):\n",
    "            #Fp\n",
    "            activations,back_prop_values = self.forward_propogation(X,self.parameters)\n",
    "            #Bp\n",
    "            self.parameters = self.back_propogation(Y,activations,self.parameters,back_prop_values,alpha)\n",
    "            #Print Cost after every 500 iters\n",
    "            if i%500==0:\n",
    "                print('Cost after iter '+str(i)+ ':' + str(self.calc_cost(activations,Y)/100))\n",
    " \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def predict(self,X):\n",
    "\n",
    "        \"\"\"\n",
    "        The predict function performs a simple feed forward of weights\n",
    "        and outputs yhat values \n",
    "\n",
    "        yhat is a list of the predicted value for df X\n",
    "        \"\"\"\n",
    "        yhat = []\n",
    "        X = self.data_clean(X)\n",
    "        X = np.transpose(np.array(X))\n",
    "        yhat = self.forward_propogation(X,self.parameters)[0][0]\n",
    "        return yhat\n",
    "\n",
    "    def CM(self,y_test,y_test_obs):\n",
    "        '''\n",
    "        Prints confusion matrix \n",
    "        y_test is list of y values in the test dataset\n",
    "        y_test_obs is list of y values predicted by the model\n",
    "\n",
    "        '''\n",
    "\n",
    "        for i in range(len(y_test_obs)):\n",
    "            if(y_test_obs[i]>0.6):\n",
    "                y_test_obs[i]=1\n",
    "            else:\n",
    "                y_test_obs[i]=0\n",
    "\n",
    "        cm=[[0,0],[0,0]]\n",
    "        fp=0\n",
    "        fn=0\n",
    "        tp=0\n",
    "        tn=0\n",
    "\n",
    "        for i in range(len(y_test)):\n",
    "            if(y_test[i]==1 and y_test_obs[i]==1):\n",
    "                tp=tp+1\n",
    "            if(y_test[i]==0 and y_test_obs[i]==0):\n",
    "                tn=tn+1\n",
    "            if(y_test[i]==1 and y_test_obs[i]==0):\n",
    "                fp=fp+1\n",
    "            if(y_test[i]==0 and y_test_obs[i]==1):\n",
    "                fn=fn+1\n",
    "        cm[0][0]=tn\n",
    "        cm[0][1]=fp\n",
    "        cm[1][0]=fn\n",
    "        cm[1][1]=tp\n",
    "\n",
    "        p= tp/(tp+fp)\n",
    "        r=tp/(tp+fn)\n",
    "        f1=(2*p*r)/(p+r)\n",
    "        acc = (tp+tn)/(tp+tn+fp+fn)\n",
    "        print(\"Confusion Matrix : \")\n",
    "        print(cm)\n",
    "        print(\"\\n\")\n",
    "        print(f\"Precision : {p}\")\n",
    "        print(f\"Recall : {r}\")\n",
    "        print(f\"F1 SCORE : {f1}\")\n",
    "        print(f\"Accuracy : {acc}\")\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('LBW_Dataset.csv')\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iter 500:0.3364010792992683\n",
      "Cost after iter 1000:0.3268530411411286\n",
      "Cost after iter 1500:0.31711644744181966\n",
      "Cost after iter 2000:0.31745011129139833\n",
      "Cost after iter 2500:0.31915828144716435\n",
      "Cost after iter 3000:0.3041142305058168\n",
      "Cost after iter 3500:0.30420576949305106\n",
      "Cost after iter 4000:0.3042183150332385\n",
      "Cost after iter 4500:0.3084040859947074\n",
      "Cost after iter 5000:0.28984028151014946\n",
      "Cost after iter 5500:0.28323680637478194\n",
      "Cost after iter 6000:0.29483535720830467\n",
      "Cost after iter 6500:0.2720841196242295\n",
      "Cost after iter 7000:0.264601385714161\n",
      "Cost after iter 7500:0.30935807364459567\n",
      "Cost after iter 8000:0.2605916440389692\n",
      "Cost after iter 8500:0.24857978911135528\n",
      "Cost after iter 9000:0.22473903944068463\n",
      "Cost after iter 9500:0.23529110413591714\n",
      "Cost after iter 10000:0.2297449928104815\n"
     ]
    }
   ],
   "source": [
    "nn.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix : \n",
      "[[7, 2], [1, 19]]\n",
      "\n",
      "\n",
      "Precision : 0.9047619047619048\n",
      "Recall : 0.95\n",
      "F1 SCORE : 0.9268292682926829\n",
      "Accuracy : 0.896551724137931\n"
     ]
    }
   ],
   "source": [
    "y_hat = nn.predict(X_test)\n",
    "nn.CM(y_test.tolist(),y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix : \n",
      "[[13, 6], [3, 45]]\n",
      "\n",
      "\n",
      "Precision : 0.8823529411764706\n",
      "Recall : 0.9375\n",
      "F1 SCORE : 0.9090909090909091\n",
      "Accuracy : 0.8656716417910447\n"
     ]
    }
   ],
   "source": [
    "y_hat = nn.predict(X_train)\n",
    "nn.CM(y_train.tolist(),y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1.,\n",
       "       1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
