{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Design of a Neural Network from scratch\n",
    "\n",
    "*************<IMP>*************\n",
    "Mention hyperparameters used and describe functionality in detail in this space\n",
    "- carries 1 mark\n",
    "'''\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "class NN: \n",
    "    parameters = list()\n",
    "    \n",
    "    def init_params(self,neuron_count_per_layer):\n",
    "        np.random.seed(10)\n",
    "        num_layers = len(neuron_count_per_layer)\n",
    "        params = [None for i in range(2*(num_layers-1))]\n",
    "        for i in range(1,num_layers):\n",
    "            params[2*i-2]=np.random.randn(neuron_count_per_layer[i], neuron_count_per_layer[i-1]) * 0.01\n",
    "            params[2*i-1]=np.zeros((neuron_count_per_layer[i], 1))\n",
    "        #print(params)\n",
    "        return params\n",
    "    \n",
    "    #Clean the data by replacing the null values with the mean/median/mode of the column\n",
    "    def data_clean(self,df):\n",
    "        df.columns = df.columns.str.strip()\n",
    "        for column in df.columns:\n",
    "            if column in ['Weight','HB','BP']:\n",
    "                df[column].fillna(value=df[column].mean(), inplace=True)\n",
    "            elif column in ['Community','Delivery phase','IFA','Education']:\n",
    "                df[column].fillna(value=df[column].mode()[0], inplace=True)\n",
    "            else:\n",
    "                df[column].fillna(value=df[column].median(), inplace=True)\n",
    "        return df\n",
    "    \n",
    "    def relu(self,Z):\n",
    "        return np.maximum(0,Z),Z\n",
    "    \n",
    "    def sigmoid(self,Z):\n",
    "        return 1/(1+np.exp(-Z)),Z\n",
    "    \n",
    "    def compute_activation(self,A,weight,bias,activation):\n",
    "        Z = weight@A + bias\n",
    "        cache1 = (A,weight,bias)\n",
    "        if activation=='relu':\n",
    "            A1,cache2 = self.relu(Z)\n",
    "        else:\n",
    "            A1,cache2 = self.sigmoid(Z)\n",
    "        return A1,(cache1,cache2)\n",
    "     \n",
    "    def compute_gradients(self,dA,vals,activation):\n",
    "        cache1,cache2 = vals\n",
    "        if activation=='relu':\n",
    "            dZ = np.array(dA,copy=True)\n",
    "            #print(cache2.shape,dA.shape)\n",
    "            dZ[cache2<=0]=0\n",
    "        if activation=='sigmoid':\n",
    "            sig = 1/(1+np.exp(-cache2))\n",
    "            dZ = dA * sig * (1-sig)\n",
    "        A_prev, W, b = cache1\n",
    "        x = A_prev.shape[1]\n",
    "        dW = 1 / x * dZ @ A_prev.T\n",
    "        db = 1 / x * np.sum(dZ, axis=1, keepdims=True)\n",
    "        dA_prev = W.T @ dZ\n",
    "        return dA_prev,dW,db\n",
    "            \n",
    "    \n",
    "    def forward_propogation(self,X,parameters):\n",
    "        num_layers = len(parameters)//2\n",
    "        back_prop_values = []\n",
    "        A = X\n",
    "        \n",
    "        for i in range(1,num_layers):\n",
    "            prev_act = A\n",
    "            weight = parameters[2*i-2]\n",
    "            bias = parameters[2*i-1]\n",
    "            A,back_prop_value = self.compute_activation(prev_act,weight,bias,activation='relu')\n",
    "            back_prop_values.append(back_prop_value)\n",
    "        \n",
    "    \n",
    "        #For Last layer i.e sigmoid \n",
    "        A,back_prop_value = self.compute_activation(A,parameters[-2],parameters[-1],activation='sigmoid')\n",
    "        back_prop_values.append(back_prop_value) \n",
    "        #print(len(back_prop_values))\n",
    "        return A,back_prop_values\n",
    "    \n",
    "    def back_propogation(self,Y,activations,parameters,back_prop_values,alpha):\n",
    "        #Computing the necessary derivatives\n",
    "        gradients = {}\n",
    "        num_layers = len(parameters)//2\n",
    "        Y.reshape(activations.shape)\n",
    "        dA = - (np.divide(Y, activations) - np.divide(1 - Y, 1 - activations))\n",
    "        #print(dA.shape)\n",
    "        vals = back_prop_values[num_layers-1]\n",
    "        gradients[\"dA\" + str(num_layers-1)], gradients[\"dW\" + str(num_layers)], gradients[\"db\" + str(num_layers)] = self.compute_gradients(dA,vals,'sigmoid')\n",
    "        for layer in reversed(range(num_layers-1)):\n",
    "            vals = back_prop_values[layer]\n",
    "            gradients[\"dA\" + str(layer)], gradients[\"dW\" + str(layer + 1)], gradients[\"db\" + str(layer + 1)] = self.compute_gradients(gradients['dA'+str(layer+1)], vals, 'relu')\n",
    "        \n",
    "        #Updating the parameters\n",
    "        for i in range(1,num_layers+1):\n",
    "            parameters[2*i-2]=parameters[2*i-2] - alpha * gradients['dW'+str(i)]\n",
    "            parameters[2*i-1]=parameters[2*i-1] - alpha * gradients['db'+str(i)]\n",
    "        \n",
    "        return parameters\n",
    "        \n",
    "\n",
    "    def calc_cost(self,A,Y):\n",
    "        return np.squeeze(-1 / len(Y) * np.sum(Y * np.log(A) + (1-Y) * np.log(1-A)))\n",
    "\n",
    "    ''' X and Y are dataframes '''\n",
    "\n",
    "    def fit(self,X,Y):\n",
    "        '''\n",
    "        Function that trains the neural network by taking x_train and y_train samples as input\n",
    "        '''\n",
    "\n",
    "        #Clean the data\n",
    "        X = self.data_clean(X)\n",
    "        #Set hyperparameters\n",
    "        num_itertations = 20000\n",
    "        alpha = 0.1\n",
    "        \n",
    "    \n",
    "        #Init parameters\n",
    "        neuron_count_per_layer = [9,30,30,25,1]\n",
    "        self.parameters = self.init_params(neuron_count_per_layer)\n",
    "        \n",
    "        #Making necessary changes to dimensions\n",
    "        X = np.transpose(np.array(X))\n",
    "        Y = np.array(Y)\n",
    "        Y = np.reshape(Y,(1,Y.shape[0]))\n",
    "        \n",
    "        for i in range(1,num_itertations+1):\n",
    "            #Fp\n",
    "            activations,back_prop_values = self.forward_propogation(X,self.parameters)\n",
    "            #Bp\n",
    "            self.parameters = self.back_propogation(Y,activations,self.parameters,back_prop_values,alpha)\n",
    "            #Print Cost after every 500 iters\n",
    "            if i%500==0:\n",
    "                print('Cost after iter '+str(i)+ ':' + str(self.calc_cost(activations,Y)/100))\n",
    " \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "    def predict(self,X):\n",
    "\n",
    "        \"\"\"\n",
    "        The predict function performs a simple feed forward of weights\n",
    "        and outputs yhat values \n",
    "\n",
    "        yhat is a list of the predicted value for df X\n",
    "        \"\"\"\n",
    "        yhat = []\n",
    "        X = self.data_clean(X)\n",
    "        X = np.transpose(np.array(X))\n",
    "        yhat = self.forward_propogation(X,self.parameters)[0][0]\n",
    "        return yhat\n",
    "\n",
    "    def CM(self,y_test,y_test_obs):\n",
    "        '''\n",
    "        Prints confusion matrix \n",
    "        y_test is list of y values in the test dataset\n",
    "        y_test_obs is list of y values predicted by the model\n",
    "\n",
    "        '''\n",
    "\n",
    "        for i in range(len(y_test_obs)):\n",
    "            if(y_test_obs[i]>0.6):\n",
    "                y_test_obs[i]=1\n",
    "            else:\n",
    "                y_test_obs[i]=0\n",
    "\n",
    "        cm=[[0,0],[0,0]]\n",
    "        fp=0\n",
    "        fn=0\n",
    "        tp=0\n",
    "        tn=0\n",
    "\n",
    "        for i in range(len(y_test)):\n",
    "            if(y_test[i]==1 and y_test_obs[i]==1):\n",
    "                tp=tp+1\n",
    "            if(y_test[i]==0 and y_test_obs[i]==0):\n",
    "                tn=tn+1\n",
    "            if(y_test[i]==1 and y_test_obs[i]==0):\n",
    "                fp=fp+1\n",
    "            if(y_test[i]==0 and y_test_obs[i]==1):\n",
    "                fn=fn+1\n",
    "        cm[0][0]=tn\n",
    "        cm[0][1]=fp\n",
    "        cm[1][0]=fn\n",
    "        cm[1][1]=tp\n",
    "\n",
    "        p= tp/(tp+fp)\n",
    "        r=tp/(tp+fn)\n",
    "        f1=(2*p*r)/(p+r)\n",
    "        acc = (tp+tn)/(tp+tn+fp+fn)\n",
    "        print(\"Confusion Matrix : \")\n",
    "        print(cm)\n",
    "        print(\"\\n\")\n",
    "        print(f\"Precision : {p}\")\n",
    "        print(f\"Recall : {r}\")\n",
    "        print(f\"F1 SCORE : {f1}\")\n",
    "        print(f\"Accuracy : {acc}\")\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('LBW_Dataset.csv')\n",
    "X = df.iloc[:, :-1]\n",
    "y = df.iloc[:, -1]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\sreya\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py:6287: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._update_inplace(new_data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iter 500:0.3682968077652367\n",
      "Cost after iter 1000:0.3682955293100914\n",
      "Cost after iter 1500:0.3682929699422447\n",
      "Cost after iter 2000:0.36828661465116563\n",
      "Cost after iter 2500:0.3682620672890337\n",
      "Cost after iter 3000:0.36789320908090384\n",
      "Cost after iter 3500:0.3424982073550702\n",
      "Cost after iter 4000:0.32564167647691705\n",
      "Cost after iter 4500:0.3219141704556474\n",
      "Cost after iter 5000:0.3201311566045345\n",
      "Cost after iter 5500:0.31930786352478824\n",
      "Cost after iter 6000:0.3229039699637134\n",
      "Cost after iter 6500:0.3218940108919681\n",
      "Cost after iter 7000:0.32113188249799174\n",
      "Cost after iter 7500:0.32054153046488104\n",
      "Cost after iter 8000:0.32008941979330197\n",
      "Cost after iter 8500:0.3197252877883235\n",
      "Cost after iter 9000:0.31931730986699336\n",
      "Cost after iter 9500:0.3163010581025377\n",
      "Cost after iter 10000:0.32721107210926226\n",
      "Cost after iter 10500:0.3183900398769859\n",
      "Cost after iter 11000:0.2979508781165845\n",
      "Cost after iter 11500:0.2837805023432525\n",
      "Cost after iter 12000:0.29827402937093367\n",
      "Cost after iter 12500:0.27893235145031503\n",
      "Cost after iter 13000:0.2804915949991285\n",
      "Cost after iter 13500:0.2581610934530964\n",
      "Cost after iter 14000:0.2587736956760767\n",
      "Cost after iter 14500:0.24277174909050483\n",
      "Cost after iter 15000:0.2419080559495922\n",
      "Cost after iter 15500:0.26654235391892944\n",
      "Cost after iter 16000:0.2364292740279166\n",
      "Cost after iter 16500:0.23184604320388616\n",
      "Cost after iter 17000:0.2535998441437554\n",
      "Cost after iter 17500:0.33542381707119534\n",
      "Cost after iter 18000:0.33520504049860905\n",
      "Cost after iter 18500:0.3020358745669057\n",
      "Cost after iter 19000:0.29313245100440743\n",
      "Cost after iter 19500:0.28494808339777594\n",
      "Cost after iter 20000:0.29279440451984906\n"
     ]
    }
   ],
   "source": [
    "nn.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.80100475 0.79881257 0.66726694 0.99948727 0.81413852 0.47954455\n",
      " 0.76898054 0.66722584 0.78910696 0.9782668  0.73144956 0.65565302\n",
      " 0.81095331 0.00143752 0.34522925 0.44485974 0.66794759 0.74948322\n",
      " 0.80984317 0.31479799 0.99999999 0.99961394 0.70936225 0.81741154\n",
      " 0.78745562 0.83659948 0.1521929  0.93460202 0.66920173]\n"
     ]
    }
   ],
   "source": [
    "y_hat = nn.predict(X_test)\n",
    "print(y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix : \n",
      "[[4, 2], [4, 19]]\n",
      "\n",
      "\n",
      "Precision : 0.9047619047619048\n",
      "Recall : 0.8260869565217391\n",
      "F1 SCORE : 0.8636363636363636\n",
      "Accuracy : 0.7931034482758621\n"
     ]
    }
   ],
   "source": [
    "nn.CM(y_test.tolist(),y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1., 1., 0., 1., 1., 1., 1., 1., 1., 1., 0., 0., 0., 1.,\n",
       "       1., 1., 0., 1., 1., 1., 1., 1., 1., 0., 1., 1.])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
